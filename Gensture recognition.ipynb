{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n",
      "Nothing detected!\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\imgproc\\src\\contours.cpp:197: error: (-210:Unsupported format or combination of formats) [Start]FindContours supports only CV_8UC1 images when mode != CV_RETR_FLOODFILL otherwise supports CV_32SC1 images only in function 'cvStartFindContours_Impl'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-56a37de63c87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[1;31m# This segments the hand region.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m             \u001b[0mhand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msegment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhsv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[1;31m# This checks whether hand region is segmented\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-56a37de63c87>\u001b[0m in \u001b[0;36msegment\u001b[1;34m(image, threshold)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m# This finds any available contours in the current\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m# image.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mcnts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindContours\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthresholded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETR_EXTERNAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCHAIN_APPROX_SIMPLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# This returns none if no contours detected, or\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\imgproc\\src\\contours.cpp:197: error: (-210:Unsupported format or combination of formats) [Start]FindContours supports only CV_8UC1 images when mode != CV_RETR_FLOODFILL otherwise supports CV_32SC1 images only in function 'cvStartFindContours_Impl'\n"
     ]
    }
   ],
   "source": [
    "#Importing required libraries\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split # Helps with organizing data for training\n",
    "from sklearn.metrics import confusion_matrix # Helps present results as a confusion-matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Flatten\n",
    "\n",
    "#Declaring global variables\n",
    "\n",
    "bg = None   #This is the variable for the first frame.\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# This function will compute the running average between\n",
    "# the background model and the current frame.\n",
    "#-------------------------------------------------------\n",
    "\n",
    "def run_avg(image, aWeight):\n",
    "    global bg\n",
    "    # Here, we need to take into account the first frame.\n",
    "    if bg is None:\n",
    "        bg = image.copy().astype(\"float\")\n",
    "        return\n",
    "\n",
    "    # We use this OpenCV function to compute the running\n",
    "    # average of the background models against the\n",
    "    # current frame.\n",
    "    cv2.accumulateWeighted(image, bg, aWeight)\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# This function will segment the hands in the current\n",
    "# image.\n",
    "#-------------------------------------------------------\n",
    "\n",
    "def segment(image, threshold=25):\n",
    "    global bg\n",
    "    # This finds the difference between the background\n",
    "    # and the current frame. \n",
    "    diff = cv2.absdiff(bg.astype(\"uint8\"), image)\n",
    "\n",
    "    # This uses the threshold so we can extract the\n",
    "    # foreground from the image.\n",
    "    thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # This finds any available contours in the current\n",
    "    # image.\n",
    "    (cnts, _) = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # This returns none if no contours detected, or\n",
    "    # returns results if they are.\n",
    "    if len(cnts) == 0:\n",
    "        return\n",
    "    else:\n",
    "        # This picks the object with the most contours,\n",
    "        #which should be the hand.\n",
    "        segmented = max(cnts, key=cv2.contourArea)\n",
    "        return (thresholded, segmented)\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# This function is used more for debugging and showing\n",
    "# results later. It plots the image into the notebook.\n",
    "#-------------------------------------------------------\n",
    "\n",
    "def plot_image(path):\n",
    "  img = cv2.imread(path) # Reads the image into a numpy.array\n",
    "  img_cvt = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Converts into the correct colorspace (RGB)\n",
    "  print(img_cvt.shape) # Prints the shape of the image just to check\n",
    "  plt.grid(False) # Without grid so we can see better\n",
    "  plt.imshow(img_cvt) # Shows the image\n",
    "  plt.xlabel(\"Width\")\n",
    "  plt.ylabel(\"Height\")\n",
    "  plt.title(\"Image \" + path)\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# This is the main function that will use the previously\n",
    "# defined functions to capture image.\n",
    "#-------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # This is an arbitrary value to begin the running\n",
    "    # average and should change fairly quickly.\n",
    "    aWeight = 0.5\n",
    "\n",
    "    # This finds the webcam.\n",
    "    camera = cv2.VideoCapture(0)\n",
    "\n",
    "    # This sets a Region of Interest (ROI).\n",
    "    top, right, bottom, left = 10, 350, 225, 590\n",
    "\n",
    "    # This simply initializes the number of frames.\n",
    "    num_frames = 0\n",
    "\n",
    "    #Loads the model we built in our training script.\n",
    "    model = keras.models.load_model(\"handrecognition_model.h5\")\n",
    "\n",
    "    # This loop will run until it is interrupted.\n",
    "    while(True):\n",
    "        # This gets the current frame or image.\n",
    "        (grabbed, frame) = camera.read()\n",
    "\n",
    "        kernel = np.ones((3,3),np.uint8)\n",
    "\n",
    "        # This resizes the frame for our purposes.\n",
    "        frame = imutils.resize(frame, width=700)\n",
    "\n",
    "        # This flips the frame. \n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # This makes a copy of the frame. \n",
    "        clone = frame.copy()\n",
    "\n",
    "        # This gets the height and width of the frame.\n",
    "        (height, width) = frame.shape[:2]\n",
    "\n",
    "        # This gets the new ROI.\n",
    "        roi = frame[top:bottom, right:left]\n",
    "\n",
    "        # This segment will convert the RoI to grayscale and blur it.\n",
    "        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "        #gray = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "\n",
    "        # define range of skin color in HSV\n",
    "        lower_skin = np.array([0,20,70], dtype=np.uint8)\n",
    "        upper_skin = np.array([20,255,255], dtype=np.uint8)\n",
    "\n",
    "        #extract skin color image\n",
    "        mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "\n",
    "        #extrapolate the hand to fill dark spots within\n",
    "        mask = cv2.dilate(mask,kernel,iterations = 4)\n",
    "\n",
    "        #blur the image\n",
    "        mask = cv2.GaussianBlur(mask,(5,5),100)\n",
    "        mask = cv2.resize(mask,(128,128))\n",
    "        img_array = np.array(mask)\n",
    "\n",
    "        # This calibrates our running average model until a\n",
    "        # threshold is reached.\n",
    "        if num_frames < 30:\n",
    "            run_avg(hsv, aWeight)\n",
    "        else:\n",
    "            # This segments the hand region.\n",
    "            hand = segment(hsv)\n",
    "\n",
    "            # This checks whether hand region is segmented\n",
    "            if hand is not None:\n",
    "                # If so, unpack the thresholded image and\n",
    "                # segmented region.\n",
    "                (thresholded, segmented) = hand\n",
    "\n",
    "                # This draws the segmented region and displays the frame.\n",
    "                cv2.drawContours(clone, [segmented + (right, top)], -1, (0, 0, 255))\n",
    "                cv2.imshow(\"Thesholded\", thresholded)\n",
    "\n",
    "        # This draws the segmented hand.\n",
    "        cv2.rectangle(clone, (left, top), (right, bottom), (0,255,0), 2)\n",
    "\n",
    "        # This increments the number of frames.\n",
    "        num_frames += 1\n",
    "\n",
    "        # This displays the frame with segmented hand.\n",
    "        frame_analysis = cv2.imshow(\"Video Feed\", clone)\n",
    "\n",
    "        # Changing dimension from 128x128 to 128x128x3\n",
    "        img_array = np.stack((img_array,)*3, axis=-1)\n",
    "        \n",
    "        #Our keras model used a 4D tensor, (images x height x width x channel)\n",
    "        #So changing dimension 128x128x3 into 1x128x128x3 \n",
    "        img_array_ex = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "        #This analyzes the frame to interpret the gesture.\n",
    "        try:\n",
    "            prediction = model.predict(img_array_ex)\n",
    "        except:\n",
    "            prediction = \"Nothing detected!\"\n",
    "\n",
    "        #This outputs the interpreted gesture if captured\n",
    "        print(prediction)\n",
    "\n",
    "        # This will capture any key pressed by the user.\n",
    "        keypress = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # If the user pressed \"q\", then stop looping!\n",
    "        if keypress == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "# This section frees up memory after the script is halted.\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"handrecognition_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
